# PaliGemma-VLM

**PaliGemma-VLM (Vision-Language Model)** is an end-to-end implementation of a multimodal transformer that integrates both **vision** and **language understanding**.  
This repository provides a complete reimplementation of the modelâ€™s internal architecture, built from scratch, including all the fundamental components required for both **training** and **inference**.

## ðŸš€ Features

- **RMSNorm** â€” lightweight normalization improving training stability.  
- **Attention Mechanisms** â€” includes full self-attention and cross-attention with multi-head support.  
- **Encoderâ€“Decoder Architecture** â€” modular transformer blocks designed for visual and textual processing.  
- **Feedforward Layers (MLP)** â€” custom implementations with gated activations (e.g., GELU, SwiGLU).  
- **KV Cache** â€” optimized keyâ€“value caching for efficient autoregressive decoding.  
- **Vision Encoder** â€” extracts and processes image embeddings via convolutional or patch-based inputs.  
- **Text Decoder** â€” generates coherent text conditioned on visual embeddings.  
- **Tokenizers & Projection Layers** â€” seamlessly bridge the vision and text modalities.  

## ðŸ§  Purpose

The goal of this project is to **explore and reproduce the internal mechanisms of modern vision-language models**â€”such as CLIP, Flamingo, and Geminiâ€”by manually implementing and analyzing every module to gain a deeper understanding of their design and functionality.

## ðŸ§© Tech Stack

- **Python**
- **PyTorch**
- **NumPy**
- **Hugging Face Tokenizers** (optional)

## ðŸ§ª Future Work

- Integrate pretrained weights for benchmarking  
- Add fine-tuning pipelines for custom multimodal datasets  
- Implement evaluation metrics (BLEU, CIDEr, CLIPScore)  
- Extend support for **mixed-precision (FP16)** and **distributed training**
